# Typing Clients Ingestion Pipeline - Essential Files Manifest

**Date**: July 26, 2025  
**Purpose**: Documentation of essential files required for the core workflow execution  
**Based on**: COMPLETE_WORKFLOW_EXECUTION_MAP.md analysis

## ğŸ¯ Core Workflow Files (ESSENTIAL - DO NOT DELETE)

### **Main Entry Point**
```
simple_workflow.py               # Main 6-step pipeline orchestrator
```

### **Configuration**
```
config/
â””â”€â”€ config.yaml                 # Central configuration file
```

### **Core Processing Scripts**
```
core/
â”œâ”€â”€ fix_s3_extensions.py        # S3 extension fixing utility
â”œâ”€â”€ process_pending_metadata_downloads.py  # Metadata processing
â””â”€â”€ stream_folder_contents_direct.py       # Direct streaming utility
```

### **Essential Utilities** (Based on workflow execution map)
```
utils/
â”œâ”€â”€ config.py                   # Configuration management
â”œâ”€â”€ csv_manager.py              # CSV operations & S3 integration
â”œâ”€â”€ csv_s3_versioning.py        # S3 versioning support
â”œâ”€â”€ download_drive.py           # Google Drive operations
â”œâ”€â”€ download_youtube.py         # YouTube downloading
â”œâ”€â”€ error_handling.py           # Error handling infrastructure
â”œâ”€â”€ extract_links.py            # Link extraction and scraping
â”œâ”€â”€ file_lock.py                # File locking for atomic operations
â”œâ”€â”€ http_pool.py                # HTTP connection pooling
â”œâ”€â”€ logging_config.py           # Logging configuration
â”œâ”€â”€ patterns.py                 # Regex patterns & WebDriver
â”œâ”€â”€ retry_utils.py              # Retry logic
â”œâ”€â”€ row_context.py              # Row context management
â”œâ”€â”€ s3_manager.py               # S3 operations (with virus scan handling)
â”œâ”€â”€ sanitization.py             # Data sanitization
â””â”€â”€ streaming_integration.py    # S3 streaming orchestration (NEW)
```

### **Essential Directories**
```
outputs/
â”œâ”€â”€ output.csv                  # Main structured dataset
â””â”€â”€ output.csv.backup_*         # Keep 5 most recent backups only

logs/
â”œâ”€â”€ runs/latest                 # Latest execution logs
â””â”€â”€ runs/[TODAY'S DATE]_*      # Keep current day's logs only

downloads/                      # Download cache (auto-managed)
```

### **State Files**
```
extraction_progress.json        # Progress tracking
failed_extractions.json         # Failed extraction tracking
```

## ğŸ“ Total Essential Files: ~25 files

## ğŸ—‘ï¸ Files Removed During Cleanup

### **Test & Temporary Files** (Deleted)
- `sam_torode_*.txt` (3 files)
- `sam_torode_*.json` (2 files) 
- `client_file_mapping_*.csv/.json` (4 files)
- `pipeline_state_*.json` (3 files)
- `reprocessing_candidates_*.json` (3 files)
- `s3_*_report.json` (2 files)
- `s3_inventory_*.*` (2 files)
- `s3_bucket_scan_results.json`
- `temp_s3_upload.csv`
- `upload_plan_*.json`
- `metadata_download_progress.json`
- `test_versioning.csv.lock`
- `simple_workflow.py.backup`
- Cache directories: `.pytest_cache/`, `__pycache__/`, `utils/__pycache__/`

### **Unused Utility Files** (Deleted)
- `utils/atomic_csv.py`
- `utils/cleanup_manager.py`
- `utils/cli_parser.py`
- `utils/csv_tracker.py`
- `utils/database_manager.py`
- `utils/download_utils.py`
- `utils/downloader.py`
- `utils/exceptions.py`
- `utils/google_docs_http.py`
- `utils/import_utils.py`
- `utils/logger.py`
- `utils/master_scraper.py`
- `utils/monitoring.py`
- `utils/parallel_processor.py`
- `utils/path_setup.py`
- `utils/rate_limiter.py`
- `utils/scrape_google_sheets.py`
- `utils/streaming_csv.py`
- `utils/validation.py`

### **Archived Directories** (Moved to archived/)
- `analysis/` - Analysis outputs
- `audit/` - Audit files
- `backup/` - Old backups
- `benchmark_outputs/` - Benchmark results
- `documentation/` - Pattern documentation
- `e2e_test_outputs/` - Test outputs
- `filtered_content/` - Filtered content analysis
- `dev_ops/` - Development operations
- `remap/` - Remapping utilities
- `recovery_backups/` - Recovery backup directory
- `logs/runs/[OLD_DATES]/` - Log directories older than 7 days
- `outputs/output.csv.backup_*` - Old backup files (kept 5 most recent)

## ğŸ“Š Cleanup Results

### **Before Cleanup**
- Total files: 459
- Python files: 210
- Total size: ~300+ MB

### **After Cleanup**
- Essential files: ~25
- Space freed: 0.4 MB (deleted files)
- Space archived: 299.5 MB (moved to archived/)
- Reduction: ~94% fewer files in active workspace

## ğŸ”— Integration Points

### **Main Workflow Dependencies**
```
simple_workflow.py
â”œâ”€â”€ utils/config.py â†’ config/config.yaml
â”œâ”€â”€ utils/csv_manager.py â†’ outputs/output.csv
â”œâ”€â”€ utils/extract_links.py â†’ Document scraping
â”œâ”€â”€ utils/streaming_integration.py â†’ S3 operations
â””â”€â”€ utils/s3_manager.py â†’ Direct streaming with virus scan handling
```

### **S3 Streaming Pipeline**
```
utils/streaming_integration.py
â”œâ”€â”€ utils/s3_manager.py â†’ Direct streaming operations
â”œâ”€â”€ utils/download_drive.py â†’ Drive folder listing
â””â”€â”€ utils/download_youtube.py â†’ YouTube processing
```

## ğŸ›¡ï¸ Safety Features

### **Backup Strategy**
- All deleted files archived before removal
- Archive location: `archived/cleanup_[TIMESTAMP]/`
- Git status checked before execution
- Dry-run mode by default

### **Essential File Validation**
- Pre-cleanup validation ensures all workflow files exist
- Post-cleanup validation available
- MANIFEST.md serves as recovery reference

## ğŸš€ Usage

### **Verify Current State**
```bash
# Count current files
find . -maxdepth 3 -type f | wc -l

# Check essential files exist
python cleanup_codebase.py  # Dry run mode
```

### **Execute Cleanup**
```bash
# Safe cleanup with archiving
python cleanup_codebase.py --execute
```

### **Restore Files** (if needed)
```bash
# Files are archived in timestamped directory
ls archived/cleanup_*/
```

This manifest ensures the pipeline remains fully functional with only essential files, maintaining a clean and maintainable codebase while preserving all critical functionality documented in the COMPLETE_WORKFLOW_EXECUTION_MAP.md.